{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "#  Embedding層の引数は少なくとも２つ\n",
    "# 有効なトークンの数：　この場合は1,000（１＋単語のインデックスの最大値）\n",
    "# 埋め込みの次元の数：この場合は64\n",
    "\n",
    "embedding_layer = Embedding(1000, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding層で使用するIMdbデータセットを読み込む\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "# 特徴量として考慮する単語の数\n",
    "max_features = 10000\n",
    "\n",
    "# max_features個の最も出現頻度の高い単語のうち，\n",
    "# この数の単語を残してテキストをカット\n",
    "max_len = 20\n",
    "\n",
    "# データを複数の整数リストとして読み込む\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 整数のリストを形状が(samples, max_len)の二次元整数テンソルに変換\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 4s 224us/sample - loss: 0.6716 - acc: 0.6133 - val_loss: 0.6228 - val_acc: 0.6968\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 3s 147us/sample - loss: 0.5439 - acc: 0.7520 - val_loss: 0.5292 - val_acc: 0.7258\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 3s 142us/sample - loss: 0.4611 - acc: 0.7865 - val_loss: 0.5030 - val_acc: 0.7438\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 3s 142us/sample - loss: 0.4201 - acc: 0.8098 - val_loss: 0.4984 - val_acc: 0.7502\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 4s 180us/sample - loss: 0.3922 - acc: 0.8250 - val_loss: 0.4968 - val_acc: 0.7514\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 3s 148us/sample - loss: 0.3694 - acc: 0.8391 - val_loss: 0.5023 - val_acc: 0.7526\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.3488 - acc: 0.8493 - val_loss: 0.5065 - val_acc: 0.7486\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 3s 170us/sample - loss: 0.3302 - acc: 0.8603 - val_loss: 0.5164 - val_acc: 0.7480\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 3s 166us/sample - loss: 0.3123 - acc: 0.8717 - val_loss: 0.5225 - val_acc: 0.7466\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.2962 - acc: 0.8802 - val_loss: 0.5324 - val_acc: 0.7462\n"
     ]
    }
   ],
   "source": [
    "# IMdbデータでEmbeddingそうと分類器を使用\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 後から埋め込みの入力を平坦化できるように\n",
    "# Embedding層の入力の長さとしてmax_lenを指定\n",
    "# Embedding層の後，活性化の形状は(samples, max_len, 8)になる\n",
    "model.add(Embedding(10000, 8, input_length=max_len))\n",
    "\n",
    "# 埋め込みの三次元テンソルを形状が(samples, max_len * 8)の二次元テンソルに変換\n",
    "model.add(Flatten())\n",
    "\n",
    "# 最後に分類機を追加\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train,\n",
    "                   epochs=10,\n",
    "                   batch_size=32,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('alphago': conda)",
   "language": "python",
   "name": "python37564bitalphagocondab5db145d4df74371a4045b800489042b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
